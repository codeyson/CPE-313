{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wDlWLbfkJtvu"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2020 Google LLC. Double-click here for license information.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Linear Regression with a Real Dataset\n",
        "\n",
        "This Colab uses a real dataset to predict the prices of houses in California.   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wtceyJj2uX"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "After doing this Colab, you'll know how to do the following:\n",
        "\n",
        "  * Read a .csv file into a [pandas](https://developers.google.com/machine-learning/glossary/#pandas) DataFrame.\n",
        "  * Examine a [dataset](https://developers.google.com/machine-learning/glossary/#data_set).\n",
        "  * Experiment with different [features](https://developers.google.com/machine-learning/glossary/#feature) in building a model.\n",
        "  * Tune the model's [hyperparameters](https://developers.google.com/machine-learning/glossary/#hyperparameter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZEgJQSjyK4"
      },
      "source": [
        "## The Dataset\n",
        "  \n",
        "The [dataset for this exercise](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) is based on 1990 census data from California. The dataset is old but still provides a great opportunity to learn about machine learning programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchnxAsaKKqO"
      },
      "source": [
        "## Import relevant modules\n",
        "\n",
        "The following hidden code cell imports the necessary code to run the code in the rest of this Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Keras (Python 3.11.8)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python312\\lib\\site-packages (from tensorflow) (25.9.23)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Using cached gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\python312\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\python312\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in c:\\python312\\lib\\site-packages (from tensorflow) (6.33.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from tensorflow) (69.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Using cached grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.10.0 (from tensorflow)\n",
            "  Using cached keras-3.13.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.26.4)\n",
            "Collecting h5py>=3.11.0 (from tensorflow)\n",
            "  Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\python312\\lib\\site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Collecting rich (from keras>=3.10.0->tensorflow)\n",
            "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.0.7)\n",
            "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\python312\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.2.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python312\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
            "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
            "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
            "Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
            "Using cached keras-3.13.0-py3-none-any.whl (1.5 MB)\n",
            "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Installing collected packages: markdown-it-py, h5py, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, rich, keras, tensorflow\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~%p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~=p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
            "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "WARNING: Ignoring invalid distribution ~%p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~=p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
            "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
            "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python312\\\\Scripts\\\\markdown-it.exe' -> 'c:\\\\Python312\\\\Scripts\\\\markdown-it.exe.deleteme'\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "9n9_cTveKmse"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#@title Import relevant modules\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# The following lines adjust the granularity of reporting.\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "#@title Import relevant modules\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "Datasets are often stored on disk or at a URL in [.csv format](https://wikipedia.org/wiki/Comma-separated_values).\n",
        "\n",
        "A well-formed .csv file contains column names in the first row, followed by many rows of data.  A comma divides each value in each row. For example, here are the first five rows of the .csv file holding the California Housing Dataset:\n",
        "\n",
        "```\n",
        "\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"\n",
        "-114.310000,34.190000,15.000000,5612.000000,1283.000000,1015.000000,472.000000,1.493600,66900.000000\n",
        "-114.470000,34.400000,19.000000,7650.000000,1901.000000,1129.000000,463.000000,1.820000,80100.000000\n",
        "-114.560000,33.690000,17.000000,720.000000,174.000000,333.000000,117.000000,1.650900,85700.000000\n",
        "-114.570000,33.640000,14.000000,1501.000000,337.000000,515.000000,226.000000,3.191700,73400.000000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSFQkzNlj-l6"
      },
      "source": [
        "### Load the .csv file into a pandas DataFrame\n",
        "\n",
        "This Colab, like many machine learning programs, gathers the .csv file and stores the data in memory as a pandas Dataframe.  Pandas is an open source Python library.  The primary datatype in pandas is a DataFrame.  You can imagine a pandas DataFrame as a spreadsheet in which each row is identified by a number and each column by a name. Pandas is itself built on another open source Python library called NumPy. If you aren't familiar with these technologies, please view these two quick tutorials:\n",
        "\n",
        "*   [NumPy](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=numpy_tf2-colab&hl=en)\n",
        "*   [Pandas DataFrames](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=pandas_tf2-colab&hl=en)\n",
        "\n",
        "The following code cell imports the .csv file into a pandas DataFrame and scales the values in the label (`median_house_value`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -114.31     34.19                15.0       5612.0          1283.0   \n",
              "1    -114.47     34.40                19.0       7650.0          1901.0   \n",
              "2    -114.56     33.69                17.0        720.0           174.0   \n",
              "3    -114.57     33.64                14.0       1501.0           337.0   \n",
              "4    -114.57     33.57                20.0       1454.0           326.0   \n",
              "\n",
              "   population  households  median_income  median_house_value  \n",
              "0      1015.0       472.0         1.4936                66.9  \n",
              "1      1129.0       463.0         1.8200                80.1  \n",
              "2       333.0       117.0         1.6509                85.7  \n",
              "3       515.0       226.0         3.1917                73.4  \n",
              "4       624.0       262.0         1.9250                65.5  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the dataset.\n",
        "training_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "\n",
        "# Scale the label.\n",
        "training_df[\"median_house_value\"] /= 1000.0\n",
        "\n",
        "# Print the first rows of the pandas DataFrame.\n",
        "training_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5inxx49n4U9u"
      },
      "source": [
        "Scaling `median_house_value` puts the value of each house in units of thousands. Scaling will keep loss values and learning rates in a friendlier range.  \n",
        "\n",
        "Although scaling a label is usually *not* essential, scaling features in a multi-feature model usually *is* essential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMysi6-3IAbu"
      },
      "source": [
        "## Examine the dataset\n",
        "\n",
        "A large part of most machine learning projects is getting to know your data. The pandas API provides a `describe` function that outputs the following statistics about every column in the DataFrame:\n",
        "\n",
        "* `count`, which is the number of rows in that column. Ideally, `count` contains the same value for every column.\n",
        "\n",
        "* `mean` and `std`, which contain the mean and standard deviation of the values in each column.\n",
        "\n",
        "* `min` and `max`, which contain the lowest and highest values in each column.\n",
        "\n",
        "* `25%`, `50%`, `75%`, which contain various [quantiles](https://developers.google.com/machine-learning/glossary/#quantile)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rnUSYKw4LUuh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-119.562108</td>\n",
              "      <td>35.625225</td>\n",
              "      <td>28.589353</td>\n",
              "      <td>2643.664412</td>\n",
              "      <td>539.410824</td>\n",
              "      <td>1429.573941</td>\n",
              "      <td>501.221941</td>\n",
              "      <td>3.883578</td>\n",
              "      <td>207.300912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.005166</td>\n",
              "      <td>2.137340</td>\n",
              "      <td>12.586937</td>\n",
              "      <td>2179.947071</td>\n",
              "      <td>421.499452</td>\n",
              "      <td>1147.852959</td>\n",
              "      <td>384.520841</td>\n",
              "      <td>1.908157</td>\n",
              "      <td>115.983764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-124.350000</td>\n",
              "      <td>32.540000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.499900</td>\n",
              "      <td>14.999000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-121.790000</td>\n",
              "      <td>33.930000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>1462.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>790.000000</td>\n",
              "      <td>282.000000</td>\n",
              "      <td>2.566375</td>\n",
              "      <td>119.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-118.490000</td>\n",
              "      <td>34.250000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>2127.000000</td>\n",
              "      <td>434.000000</td>\n",
              "      <td>1167.000000</td>\n",
              "      <td>409.000000</td>\n",
              "      <td>3.544600</td>\n",
              "      <td>180.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-118.000000</td>\n",
              "      <td>37.720000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>3151.250000</td>\n",
              "      <td>648.250000</td>\n",
              "      <td>1721.000000</td>\n",
              "      <td>605.250000</td>\n",
              "      <td>4.767000</td>\n",
              "      <td>265.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>-114.310000</td>\n",
              "      <td>41.950000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>37937.000000</td>\n",
              "      <td>6445.000000</td>\n",
              "      <td>35682.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>15.000100</td>\n",
              "      <td>500.001000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          longitude      latitude  housing_median_age   total_rooms  \\\n",
              "count  17000.000000  17000.000000        17000.000000  17000.000000   \n",
              "mean    -119.562108     35.625225           28.589353   2643.664412   \n",
              "std        2.005166      2.137340           12.586937   2179.947071   \n",
              "min     -124.350000     32.540000            1.000000      2.000000   \n",
              "25%     -121.790000     33.930000           18.000000   1462.000000   \n",
              "50%     -118.490000     34.250000           29.000000   2127.000000   \n",
              "75%     -118.000000     37.720000           37.000000   3151.250000   \n",
              "max     -114.310000     41.950000           52.000000  37937.000000   \n",
              "\n",
              "       total_bedrooms    population    households  median_income  \\\n",
              "count    17000.000000  17000.000000  17000.000000   17000.000000   \n",
              "mean       539.410824   1429.573941    501.221941       3.883578   \n",
              "std        421.499452   1147.852959    384.520841       1.908157   \n",
              "min          1.000000      3.000000      1.000000       0.499900   \n",
              "25%        297.000000    790.000000    282.000000       2.566375   \n",
              "50%        434.000000   1167.000000    409.000000       3.544600   \n",
              "75%        648.250000   1721.000000    605.250000       4.767000   \n",
              "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
              "\n",
              "       median_house_value  \n",
              "count        17000.000000  \n",
              "mean           207.300912  \n",
              "std            115.983764  \n",
              "min             14.999000  \n",
              "25%            119.400000  \n",
              "50%            180.400000  \n",
              "75%            265.000000  \n",
              "max            500.001000  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get statistics on the dataset.\n",
        "training_df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9pcW_Yjtoo8"
      },
      "source": [
        "### Task 1: Identify anomalies in the dataset\n",
        "\n",
        "Do you see any anomalies (strange values) in the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UoS7NWRXEs1H"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view a possible answer.\n",
        "\n",
        "# The maximum value (max) of several columns seems very\n",
        "# high compared to the other quantiles. For example,\n",
        "# example the total_rooms column. Given the quantile\n",
        "# values (25%, 50%, and 75%), you might expect the\n",
        "# max value of total_rooms to be approximately\n",
        "# 5,000 or possibly 10,000. However, the max value\n",
        "# is actually 37,937.\n",
        "\n",
        "# When you see anomalies in a column, become more careful\n",
        "# about using that column as a feature. That said,\n",
        "# anomalies in potential features sometimes mirror\n",
        "# anomalies in the label, which could make the column\n",
        "# be (or seem to be) a powerful feature.\n",
        "# Also, as you will see later in the course, you\n",
        "# might be able to represent (pre-process) raw data\n",
        "# in order to make columns into useful features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Define functions that build and train a model\n",
        "\n",
        "The following code defines two functions:\n",
        "\n",
        "  * `build_model(my_learning_rate)`, which builds a randomly-initialized model.\n",
        "  * `train_model(model, feature, label, epochs)`, which trains the model from the examples (feature and label) you pass.\n",
        "\n",
        "Since you don't need to understand model building code right now, we've hidden this code cell.  You may optionally double-click the following headline to see the code that builds and trains a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "id": "pedD5GhlDC-y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined the build_model and train_model functions.\n"
          ]
        }
      ],
      "source": [
        "#@title Define the functions that build and train a model\n",
        "def build_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,\n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "  # Compile the model topography into code that TensorFlow can efficiently\n",
        "  # execute. Configure training to minimize the model's mean squared error.\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, df, feature, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the model the feature and the label.\n",
        "  # The model will train for the specified number of epochs.\n",
        "  history = model.fit(x=df[feature],\n",
        "                      y=df[label],\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the error for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # To track the progression of training, we're going to take a snapshot\n",
        "  # of the model's root mean squared error at each epoch.\n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse\n",
        "\n",
        "print(\"Defined the build_model and train_model functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Define plotting functions\n",
        "\n",
        "The following [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) functions create the following plots:\n",
        "\n",
        "*  a scatter plot of the feature vs. the label, and a line showing the output of the trained model\n",
        "*  a loss curve\n",
        "\n",
        "You may optionally double-click the headline to see the matplotlib code, but note that writing matplotlib code is not an important part of learning ML programming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cellView": "form",
        "id": "QF0BFRXTOeR3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined the plot_the_model and plot_the_loss_curve functions.\n"
          ]
        }
      ],
      "source": [
        "#@title Define the plotting functions\n",
        "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
        "  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n",
        "\n",
        "  # Label the axes.\n",
        "  plt.xlabel(feature)\n",
        "  plt.ylabel(label)\n",
        "\n",
        "  # Create a scatter plot from 200 random points of the dataset.\n",
        "  random_examples = training_df.sample(n=200)\n",
        "  plt.scatter(random_examples[feature], random_examples[label])\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = random_examples[feature].max()\n",
        "  y1 = trained_bias + (trained_weight * x1)\n",
        "  plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, rmse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "  plt.show()\n",
        "\n",
        "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD"
      },
      "source": [
        "## Call the model functions\n",
        "\n",
        "An important part of machine learning is determining which [features](https://developers.google.com/machine-learning/glossary/#feature) correlate with the [label](https://developers.google.com/machine-learning/glossary/#label). For example, real-life home-value prediction models typically rely on hundreds of features and synthetic features. However, this model relies on only one feature. For now, you'll arbitrarily use `total_rooms` as that feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "both",
        "id": "nj3v5EKQFY8s"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m my_model = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Invoke the functions.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m my_model = \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m weight, bias, epochs, rmse = train_model(my_model, training_df,\n\u001b[32m     18\u001b[39m                                          my_feature, my_label,\n\u001b[32m     19\u001b[39m                                          epochs, batch_size)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThe learned weight for your model is \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[33m\"\u001b[39m % weight)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mbuild_model\u001b[39m\u001b[34m(my_learning_rate)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create and compile a simple linear regression model.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Most simple tf.keras models are sequential.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mtf\u001b[49m.keras.models.Sequential()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Describe the topography of the model.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# The topography of a simple linear regression model\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# is a single node in a single layer.\u001b[39;00m\n\u001b[32m     10\u001b[39m model.add(tf.keras.layers.Dense(units=\u001b[32m1\u001b[39m,\n\u001b[32m     11\u001b[39m                                 input_shape=(\u001b[32m1\u001b[39m,)))\n",
            "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 30\n",
        "batch_size = 30\n",
        "\n",
        "# Specify the feature and the label.\n",
        "my_feature = \"total_rooms\"  # the total number of rooms on a specific city block.\n",
        "my_label=\"median_house_value\" # the median value of a house on a specific city block.\n",
        "# That is, you're going to create a model that predicts house value based\n",
        "# solely on total_rooms.\n",
        "\n",
        "# Discard any pre-existing version of the model.\n",
        "my_model = None\n",
        "\n",
        "# Invoke the functions.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "print(\"\\nThe learned weight for your model is %.4f\" % weight)\n",
        "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
        "\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btp8zUNbYOcd"
      },
      "source": [
        "A certain amount of randomness plays into training a model. Consequently, you'll get different results each time you train the model. That said, given the dataset and the hyperparameters, the trained model will generally do a poor job describing the feature's relation to the label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xNqWWos_zyk"
      },
      "source": [
        "## Use the model to make predictions\n",
        "\n",
        "You can use the trained model to make predictions. In practice, [you should make predictions on examples that are not used in training](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). However, for this exercise, you'll just work with a subset of the same training dataset. A later Colab exercise will explore ways to make predictions on examples not used in training.\n",
        "\n",
        "First, run the following code to define the house prediction function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH63BmncAcab"
      },
      "outputs": [],
      "source": [
        "def predict_house_values(n, feature, label):\n",
        "  \"\"\"Predict house values based on a feature.\"\"\"\n",
        "\n",
        "  batch = training_df[feature][10000:10000 + n]\n",
        "  predicted_values = my_model.predict_on_batch(x=batch)\n",
        "\n",
        "  print(\"feature   label          predicted\")\n",
        "  print(\"  value   value          value\")\n",
        "  print(\"          in thousand$   in thousand$\")\n",
        "  print(\"--------------------------------------\")\n",
        "  for i in range(n):\n",
        "    print (\"%5.0f %6.0f %15.0f\" % (training_df[feature][10000 + i],\n",
        "                                   training_df[label][10000 + i],\n",
        "                                   predicted_values[i][0] ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbBNQujU5WjK"
      },
      "source": [
        "Now, invoke the house prediction function on 10 examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_0DGBt0Kz_N"
      },
      "outputs": [],
      "source": [
        "predict_house_values(10, my_feature, my_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGaqArcpqY3"
      },
      "source": [
        "### Task 2: Judge the predictive power of the model\n",
        "\n",
        "Look at the preceding table. How close is the predicted value to the label value?  In other words, does your model accurately predict house values?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVpjhUFm9uID"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view the answer.\n",
        "\n",
        "# Most of the predicted values differ significantly\n",
        "# from the label value, so the trained model probably\n",
        "# doesn't have much predictive power. However, the\n",
        "# first 10 examples might not be representative of\n",
        "# the rest of the examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLoqis3IUPSd"
      },
      "source": [
        "## Task 3: Try a different feature\n",
        "\n",
        "The `total_rooms` feature had only a little predictive power. Would a different feature have greater predictive power?  Try using `population` as the feature instead of `total_rooms`.\n",
        "\n",
        "Note: When you change features, you might also need to change the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0ab6HD4ZO75"
      },
      "outputs": [],
      "source": [
        "my_feature = \"?\"   # Replace the ? with population or possibly\n",
        "                   # a different column name.\n",
        "\n",
        "# Experiment with the hyperparameters.\n",
        "learning_rate = 2\n",
        "epochs = 3\n",
        "batch_size = 120\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "predict_house_values(15, my_feature, my_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "107mDkW7U6mg"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view a possible solution.\n",
        "\n",
        "my_feature = \"population\" # Pick a feature other than \"total_rooms\"\n",
        "\n",
        "# Possibly, experiment with the hyperparameters.\n",
        "learning_rate = 0.05\n",
        "epochs = 18\n",
        "batch_size = 3\n",
        "\n",
        "# Don't change anything below.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "predict_house_values(10, my_feature, my_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd_rHJ59AUtk"
      },
      "source": [
        "Did `population` produce better predictions than `total_rooms`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0tPEtzcC-vK"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view the answer.\n",
        "\n",
        "# Training is not entirely deterministic, but population\n",
        "# typically converges at a slightly higher RMSE than\n",
        "# total_rooms.  So, population appears to be about\n",
        "# the same or slightly worse at making predictions\n",
        "# than total_rooms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uYpyGacsIg"
      },
      "source": [
        "## Task 4: Define a synthetic feature\n",
        "\n",
        "You have determined that `total_rooms` and `population` were not useful features.  That is, neither the total number of rooms in a neighborhood nor the neighborhood's population successfully predicted the median house price of that neighborhood. Perhaps though, the *ratio* of `total_rooms` to `population` might have some predictive power. That is, perhaps block density relates to median house value.\n",
        "\n",
        "To explore this hypothesis, do the following:\n",
        "\n",
        "1. Create a [synthetic feature](https://developers.google.com/machine-learning/glossary/#synthetic_feature) that's a ratio of `total_rooms` to `population`. (If you are new to pandas DataFrames, please study the [Pandas DataFrame Ultraquick Tutorial](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=pandas_tf2-colab&hl=en).)\n",
        "2. Tune the three hyperparameters.\n",
        "3. Determine whether this synthetic feature produces\n",
        "   a lower loss value than any of the single features you\n",
        "   tried earlier in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kx2xHSgdcpg"
      },
      "outputs": [],
      "source": [
        "# Define a synthetic feature named rooms_per_person\n",
        "training_df[\"rooms_per_person\"] = ? # write your code here.\n",
        "\n",
        "# Don't change the next line.\n",
        "my_feature = \"rooms_per_person\"\n",
        "\n",
        "# Assign values to these three hyperparameters.\n",
        "learning_rate = ?\n",
        "epochs = ?\n",
        "batch_size = ?\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "predict_house_values(15, my_feature, my_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xRfxp_3yofe3"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view a possible solution to Task 4.\n",
        "\n",
        "# Define a synthetic feature\n",
        "training_df[\"rooms_per_person\"] = training_df[\"total_rooms\"] / training_df[\"population\"]\n",
        "my_feature = \"rooms_per_person\"\n",
        "\n",
        "# Tune the hyperparameters.\n",
        "learning_rate = 0.06\n",
        "epochs = 24\n",
        "batch_size = 30\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, mae = train_model(my_model, training_df,\n",
        "                                        my_feature, my_label,\n",
        "                                        epochs, batch_size)\n",
        "\n",
        "plot_the_loss_curve(epochs, mae)\n",
        "predict_house_values(15, my_feature, my_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBiDWursB1Wi"
      },
      "source": [
        "Based on the loss values, this synthetic feature produces a better model than the individual features you tried in Task 2 and Task 3. However, the model still isn't creating great predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEG_9oU9O54u"
      },
      "source": [
        "## Task 5. Find feature(s) whose raw values correlate with the label\n",
        "\n",
        "So far, we've relied on trial-and-error to identify possible features for the model.  Let's rely on statistics instead.\n",
        "\n",
        "A **correlation matrix** indicates how each attribute's raw values relate to the other attributes' raw values. Correlation values have the following meanings:\n",
        "\n",
        "  * `1.0`: perfect positive correlation; that is, when one attribute rises, the other attribute rises.\n",
        "  * `-1.0`: perfect negative correlation; that is, when one attribute rises, the other attribute falls.\n",
        "  * `0.0`: no correlation; the two columns [are not linearly related](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg).\n",
        "\n",
        "In general, the higher the absolute value of a correlation value, the greater its predictive power. For example, a correlation value of -0.8 implies far more predictive power than a correlation of -0.2.\n",
        "\n",
        "The following code cell generates the correlation matrix for attributes of the California Housing Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFGKL45LO8Tt"
      },
      "outputs": [],
      "source": [
        "# Generate a correlation matrix.\n",
        "training_df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp0r3NAVPEdt"
      },
      "source": [
        "The correlation matrix shows nine potential features (including a synthetic\n",
        "feature) and one label (`median_house_value`).  A strong negative correlation or strong positive correlation with the label suggests a potentially good feature.  \n",
        "\n",
        "**Your Task:** Determine which of the nine potential features appears to be the best candidate for a feature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RomQTd1OPVd0"
      },
      "outputs": [],
      "source": [
        "#@title Double-click here for the solution to Task 5\n",
        "\n",
        "# The median_income correlates 0.7 with the label\n",
        "# (median_house_value), so median_income might be a\n",
        "# good feature. The other seven potential features\n",
        "# all have a correlation relatively close to 0.\n",
        "\n",
        "# If time permits, try median_income as the feature\n",
        "# and see whether the model improves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RqvEbaVSlRt"
      },
      "source": [
        "Correlation matrices don't tell the entire story. In later exercises, you'll find additional ways to unlock predictive power from potential features.\n",
        "\n",
        "**Note:** Using `median_income` as a feature may raise some ethical and fairness\n",
        "issues. Towards the end of the course, we'll explore ethical and fairness issues."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Keras",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
